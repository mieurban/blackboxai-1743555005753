input {
  # Collect logs from Docker containers
  beats {
    port => 5044
    type => "docker"
  }

  # Collect application logs
  file {
    path => "/var/log/app/*.log"
    type => "application"
    start_position => "beginning"
    sincedb_path => "/var/lib/logstash/sincedb"
    codec => json
  }

  # Collect system logs
  syslog {
    port => 5140
    type => "system"
  }

  # Collect nginx access logs
  file {
    path => "/var/log/nginx/access.log"
    type => "nginx-access"
    start_position => "beginning"
    sincedb_path => "/var/lib/logstash/sincedb_nginx"
  }

  # Collect nginx error logs
  file {
    path => "/var/log/nginx/error.log"
    type => "nginx-error"
    start_position => "beginning"
    sincedb_path => "/var/lib/logstash/sincedb_nginx_error"
  }
}

filter {
  # Parse Docker logs
  if [type] == "docker" {
    json {
      source => "message"
    }
    date {
      match => ["timestamp", "ISO8601"]
      target => "@timestamp"
    }
  }

  # Parse application logs
  if [type] == "application" {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} %{GREEDYDATA:message}" }
    }
    date {
      match => ["timestamp", "ISO8601"]
      target => "@timestamp"
    }
    mutate {
      remove_field => ["timestamp"]
    }
  }

  # Parse nginx access logs
  if [type] == "nginx-access" {
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
    }
    geoip {
      source => "clientip"
    }
    useragent {
      source => "agent"
      target => "user_agent"
    }
  }

  # Parse nginx error logs
  if [type] == "nginx-error" {
    grok {
      match => { "message" => "%{NGINX_ERROR_LOG}" }
    }
  }

  # Add common fields
  mutate {
    add_field => {
      "environment" => "${ENVIRONMENT:production}"
      "application" => "etsy-clone"
    }
  }

  # Drop health check logs
  if [request] =~ "^/health" {
    drop { }
  }
}

output {
  # Send all logs to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logs-%{+YYYY.MM.dd}"
    template_name => "logstash"
    template_overwrite => true
  }

  # Send error logs to a separate index
  if [log_level] == "ERROR" or [type] == "nginx-error" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "errors-%{+YYYY.MM.dd}"
    }
  }

  # Send metrics to Prometheus (via pushgateway)
  if [type] == "application" {
    prometheus {
      pushgateway_url => "http://pushgateway:9091"
      job => "logstash"
      metrics => {
        "log_events_total" => {
          type => "counter"
          description => "Total number of log events"
          labels => {
            "level" => "%{[log_level]}"
            "type" => "%{[type]}"
          }
        }
      }
    }
  }

  # Debug output (disabled in production)
  if "${ENVIRONMENT}" == "development" {
    stdout {
      codec => rubydebug
    }
  }
}

# Templates
template {
  name => "logstash"
  template => {
    "index_patterns" => ["logs-*"],
    "settings" => {
      "number_of_shards" => 1,
      "number_of_replicas" => 1,
      "index.refresh_interval" => "5s"
    },
    "mappings" => {
      "properties" => {
        "@timestamp" => { "type" => "date" },
        "@version" => { "type" => "keyword" },
        "message" => { "type" => "text" },
        "log_level" => { "type" => "keyword" },
        "type" => { "type" => "keyword" },
        "environment" => { "type" => "keyword" },
        "application" => { "type" => "keyword" },
        "host" => { "type" => "keyword" },
        "path" => { "type" => "keyword" },
        "tags" => { "type" => "keyword" }
      }
    }
  }
}